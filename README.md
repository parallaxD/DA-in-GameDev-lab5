# DA-in-GameDev-lab5

Отчет по лабораторной работе #5 выполнил(а):
- Куплевацкий Денис Игоревич
- РИ220931

Отметка о выполнении заданий (заполняется студентом):

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

знак "*" - задание выполнено; знак "#" - задание не выполнено;

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
  
- Цель работы

- Задание 1. Найти внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели.

- Задание 2. Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.
  
- Задание 3. Приведите примеры, для каких игровых задач и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?
  
- Выводы.

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1
### Найти внутри C# скрипта “коэффициент корреляции” и сделать выводы о том, как он влияет на обучение модели.
 
Ход работы: 
- В ходе выполнения первого задания из лабораторной работы №5 я создал сцену в Unity и реализовал систему поиска объекта агентом. Для этого я создал скрипт RollerAgent и добавил код, полученный из материалов лабораторной работы (https://drive.google.com/file/d/1vkf41XpFz8_E_reSRfKgCPDFfh_WZsX4/view).
- В контексте машинного обучения коэффициент корреляции обычно используется для измерения степени линейной зависимости между двумя переменными. Этот коэффициент может помочь определить, насколько изменения в одной переменной связаны с изменениями в другой переменной. В машинном обучении коэффициент корреляции может быть полезен для анализа данных и понимания взаимосвязей между признаками.
- В данном скрипте нет конкретного коэффициента корреляции, согласно определению. Однако, в текущем контексте, его роль может выполнять коэффициент "1.42f" в условной конструкции "if (distanceToTarget < 1.42f)".
- Переменная "distanceToTarget" является связью между агентом и целью агента, а коэффициент "1.42" является верхней границей этой переменной, влияющей на ход обучения объекта текущей задаче.
- ![image](https://github.com/parallaxD/DA-in-GameDev-lab5/assets/81700733/58e7c11d-6b63-4ba0-b570-2bd4807b5633)
- Переменная distanceToTarget в этом коде представляет собой расстояние между текущей позицией объекта (this.transform.localPosition) и целевой позицией (Target.localPosition).
- Если объект агента достигает цели (расстояние меньше 1.42f), то цель достигнута успешно, и агент получает вознаграждение. В противном случае, если агент опускается ниже определенной высоты (this.transform.localPosition.y < 0), эпизод также завершается, но без предоставления вознаграждения.
- **Вывод о его влиянии на обучение следующий:** чем меньше этот коэффициент, тем точнее будет обучаться модель, ведь расстояние, необходимое для получения вознаграждения, будет меньше. Соответственно, чем он больше, тем менее точно происходит обучение. Необходимо отметить, что значение этого коэффициента также влияет на время обучения агента (обратно-пропорциональная зависимость).


## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

Описание параметров:
batch_size:
  Определяет количество образцов, используемых в каждой итерации обучения. Больший размер может ускорить обучение, но требует больше ресурсов.

learning_rate:
  Определяет размер шага при обновлении весов модели. Высокий уровень может ускорить сходимость, но с риском нестабильности. Низкий уровень может обеспечить стабильность, но с медленной сходимостью.
  
epsilon:
  Ограничивает размер обновлений стратегии, предотвращая слишком большие изменения. Большое значение может ускорить сходимость, но с риском нестабильности.
  
buffer_size:
  Определяет количество предыдущих опытов, используемых при обучении. Увеличение может улучшить стабильность обучения, но требует больше памяти.
  
num_layer:
  Определяет сложность модели. Увеличение может помочь в аппроксимации более сложных стратегий, но с риском переобучения.
  
time_horizon:
  Определяет максимальное количество временных шагов, учитываемых при каждом обновлении. Влияет на учет долгосрочных зависимостей.

hidden_units:
  Этот параметр относится к нейронной сети и определяет количество нейронов в скрытых слоях. 

num_epoch:
  Определяет, сколько раз алгоритм PPO проходит по обучающим данным в каждой итерации обучения. Каждый проход включает в себя несколько обновлений модели.

gamma:
  Определяет важность будущих вознаграждений в целях агента. 

num_steps: 
  Определяет общее количество шагов, которые должны быть выполнены агентом до завершения процесса обучения.


**Поговорим более подробно о трёх параметрах и попробуем изменить их**

1) time_horizon:
   Определяет максимальное количество временных шагов (или действий), которые учитываются при обучении агента в рамках одного обновления. При уменьшении значения этого параметра модель может лучше справляться с ближайшими задачами и короткосрочными зависимостями, однако агент может иметь затруднения в уловлении долгосрочных зависимостей и стратегий.
   А увеличение значения может помочь агенту лучше учитывать долгосрочные зависимости и стратегии, однако задействовать для обучения гораздо больше ресурсов.
   Например, я уменьшил значение с 64 до 10, и агент RollerBall начал гораздо чаще двигаться в районе одной точки, а не в сторону цели.
   
3) epsilon:
   Используется для ограничения размера обновления стратегии на каждом шаге обучения. Это сделано для предотвращения слишком больших изменений в стратегии, которые могут привести к нестабильности обучения.
   Более высокое значение epsilon означает более разрешительный подход к обновлениям стратегии. Это может привести к более быстрой сходимости, но с риском нестабильности, особенно при больших изменениях стратегии.
   Более низкое значение epsilon делает обновления стратегии более консервативными. Это может помочь в поддержании стабильного обучения, но может потребовать большего числа итераций для сходимости.
   Когда я выставил большое значение epsilon (0.8), агент начал вести себя хаотично, а выбор стратегии стал более нестабильным. Агент то плавно двигался к цели, то резко приближался к ней, то плавно двигался в другую сторону.
   
5) learning-rate:
   Определяет размер шага, на который обновляются веса модели после каждой итерации обучения.
   Более высокий learning_rate приводит к более большим изменениям весов, что может ускорить сходимость, но может также привести к перескакиванию минимума и нестабильному обучению.
   Более низкий learning_rate обычно обеспечивает более стабильное обучение, но может потребовать большего числа итераций для достижения сходимости.
   Увеличение мной learning-rate до 3.0e-1 привёл к нестабильности обучения. Агент стал часто зацикливаться и двигаться только в одну сторону, хотя цель находилась совершенно в другом месте.


## Задание 3
### Приведите примеры, для каких игровых задач и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

Агент, обученный искать и следовать за целью может использоваться в играх, например, для преследования противником персонажа.
Агента можно обучить двигаться по определенному маршруту, чтобы симулировать жизнь неигрового персонажа. Например, в игре Kingdom Come: Deliverance NPC передвигаются и занимаются чем-то связанным с их дейстельностью в игре. К примеру, торговец сначала торгует в лавке, затем идет в бар за выпивкой, затем домой.


![image](https://github.com/parallaxD/DA-in-GameDev-lab5/assets/81700733/7c5b736b-c5cf-4e1e-bb6f-039bb7c0ed6f)  
Конрад Хаген, торговец в Ратае. Kingdom Come: Deliverance.



Кроме того, в этой же игре NPC умеют обходить игрока, то есть видят в нём препятствие, что тоже может являться примером использования ML-агента.

Использование ML-Agent'а может быть предпочтительным, например, в следующих случаях:
1) Когда сценарии игры сложны и не могут быть точно предсказаны заранее.
2) Когда среда игры может изменяться со временем, и важна способность объекта адаптироваться к этим изменениям.
3) Когда необходимо, чтобы агент автоматически обновлял свои стратегии в соответствии с новыми данными и опытом.
4) Когда предпочтительно обучать агента на основе накопленного опыта, а не вручную настраивать стратегии.

Приведенные примеры являются лишь малой частью того, где можно использовать ML-агентов, да и всё машинное обучение. Идей и вариантов использования ИИ в играх невероятно много.



## Выводы

В ходе лабораторной работы я познакомился с Unity ML-Agent, посмотрел примеры обучения и работы системы машинного обучения, построенной на этих агентах, проанализировал то, как они работают, а также реализовал системы поиска объекта на сцене и "симулятор добычи ресурсов".
Я изучил параметры конфигурации обучения модели и поэкспериментировал с ними.
Кроме того, я размышлял по поводу того, в каких игровых задачах могут использоваться ML-агенты и искуственный интеллект в целом.

| Plugin | README |
| ------ | ------ |
| Dropbox | [plugins/dropbox/README.md][PlDb] |
| GitHub | [plugins/github/README.md][PlGh] |
| Google Drive | [plugins/googledrive/README.md][PlGd] |
| OneDrive | [plugins/onedrive/README.md][PlOd] |
| Medium | [plugins/medium/README.md][PlMe] |
| Google Analytics | [plugins/googleanalytics/README.md][PlGa] |

## Powered by

**BigDigital Team: Denisov | Fadeev | Panov**
